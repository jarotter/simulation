---
title: "Tarea 4"
author: "Sergio Arnaud, Jorge Rotter"
date: "19/10/2018"
output:
  html_document: default
  pdf_document: default
header-includes:
- \usepackage{amsmath}
- \usepackage{mdsymbol}
- \DeclareUnicodeCharacter{2212}{-}
- \DeclareUnicodeCharacter{301}{´}
- \DeclareUnicodeCharacter{223C}{~}
---

## Pregunta 1.

Consideren el siguiente modelo de líneas de espera con un servidor. Los tiempos de interarribo, así como los tiempos de servicio son independientes $U(0, 1)$.

Sea $A_i$ el tiempo de interarribo entre los clientes $i − 1$ e $i$ y $S_i$ es el tiempo de servicio del cliente $i$. $W_i$ es el tiempo de espera en fila del cliente $i$. La condición inicial es que el primer cliente en el sistema llega en el tiempo 0. Entonces $$W_i = max\{0, W_{i−1} + S_{i−1} − A_i\}$$ para $i = 2, 3, . . . , 100$ donde $W_1 = 0$. Escriban un programa para simular 5000 realizaciones del tiempo total de espera en la fila, junto con 5000 realizaciones antitéticas.

Comenzamos por crear la siguiente función que simula n realizaciones del proceso mencionado, con $S_i \sim U(a,b)$ y $A_i \sim U(a,b)$ donde se tiene que el parámetros "anthitecs" indica si la simulación se llevará a cabo o no con variables antitéticas:
```{r, message=FALSE,echo=FALSE, warning=FALSE}
library(tidyverse)
library(combinat)
```

```{r}
question_1_simulation <- function(n,a,b,antithetics=FALSE){
  
  tiempos_espera = rep(0,n)
    
  for (k in 1:n){
    if (antithetics){
      A = runif(50,a,b)
      A = c(A,1-A)
      S = runif(50,a,b)
      S = c(S,1-S)
    }
    else{
      A = runif(100,a,b)
      S = runif(100,a,b)
    }
    W = rep(0,100)
    for (i in 2:100){
      W[i] = max(0, W[i-1] + S[i-1] - A[i])
    }
    tiempos_espera[k] = sum(W)
  }
  
  return (tiempos_espera)
}
```

Usando un estimador combinado de las realizaciones primarias y antitéticas,
estimar la esperanza del tiempo de espera de los 100 clientes y su error estándar
estimado. Estimar el porcentaje de reducción de varianza.

Realizando la simulación con variables antitéticas:
```{r}
n <- 10000
tiempos_espera_1 <- question_1_simulation(n,0,1,antithetics=TRUE)
```
La esperanza del tiempo de espera está dada por:
```{r}
mean(tiempos_espera_1)
```
Y el porcentaje de recucción de varianza:
```{r}
tiempos_espera_2 <- question_1_simulation(n,0,1,antithetics=FALSE)
100*(sd(tiempos_espera_1) - sd(tiempos_espera_2) )/sd(tiempos_espera_2)
```

Modificando a uniformes en el (0,2) obtendremos la estimación
```{r}
tiempos_espera_1 <- question_1_simulation(n,0,2,antithetics=FALSE)
tiempos_espera_2 <- question_1_simulation(n,0,2,antithetics=TRUE)
mean(tiempos_espera_2)
```

Con una reducción de varianza:
```{r}
100*(sd(tiempos_espera_2) - sd(tiempos_espera_1)) / (sd(tiempos_espera_1))
```

## Pregunta 2
Cinco elementos, numerados del 1 al 5 se acomodan inicialmente en un orden aleatorio. En cada estado del proceso, uno de los elementos es seleccionado y puesto en el frente de la lista. Por ejemplo, si el orden presente es $(2,3,4,1,5)$ y el elemento 1 se elige, entonces el nuevo orden es $(1,2,3,4,5)$. Supongan que cada selección es, independientemente, elemento $i$ con probabilidad $pi$, donde

\[
\mathbf~{p}'=(\frac{1}{15},\frac{2}{15},\frac{3}{15},\frac{4}{15},\frac{5}{15},)
\]

Sea $L_j$ la posición del $j$-ésimo elemento seleccionado y $L=\sum_{j=1}^{100}L_j$

a. Explique cómo utilizar simulación para estimar $\mathbb{E}[L]$.

Recordemos que hay $5!=120$ permutaciones de los números $\{1, 2, 3, 4, 5\}$. Podemos enumerarlas (por ejemplo en el orden que `combinat::permn` las regresa en `R`) y elegir una es equivalente a extraer una muestra tamaño uno de $P \sim \mathcal{U}\{1,5\}$ (discreta). (Nótese que esta es la parte tardada de todo el proceso, por lo que es mejor generarlas sólo cuando se necesiten e irlas guardando en memoria).

Una vez elegida la distribución inicial, elegimos una muestra tamaño $n=100$ de con la masa de probabilidad $\mathbf{p}$ de arriba. Cada observación es un cambio. Vamos haciendo los cambios en el orden de la muestra, y entre cambio y cambio tenemos la $L_j$ correspondiente. Con este proceso obtenemos las 100 $L_j$, que luego usamos para calcular $L$. Repitiendo el proceso un número grande de veces, aproximamos $\mathbb{E}[L]$ con $\bar{L}$.

b. Calcule $\mathbb{E}[N_i]$, donde $N_i$ es el número de veces que se elige al elemento $i$ en 100.

Notemos que, como las elecciones son independientes, $N_i \sim \textrm{Bin}(100,p_i)$, por lo que $\mathbb{E}[N_i]=100p_i$.

c. Sea $Y = \sum_{i=1}^5iN_i$. ¿Cómo se correlaciona $Y$ con $L$?

$$
\begin{aligned}
\mathbb{E}[Y] &= \mathbb{E}\left[\sum_{i=1}^5Ni\right] = \sum_{i=1}^5i\mathbb{E}[Ni] = \sum_{i=1}^5100ip_i = 100\sum_{i=1}^5ip_i=100\mathbb{E}[L_1] \\ & =\sum_{j=1}^{100}\mathbb{E}[L_1]= \sum_{j=1}^{100}\mathbb{E}[L_j]  = \mathbb{E}\left[\sum_{j=1}^{100}Lj\right] = \mathbb{E}[L]
\end{aligned}
$$

d. Desarrolle un estudio para estimar $L$ usando $Y$ como variable de control.

```{r}
distribuciones_iniciales <- function(n_sims){
    permutacion <- permn(1:5)
    inicial <- base::sample(x=1:120, size=n_sims, replace=TRUE)
    inicial <- permutacion[inicial]
    inicial
}


actualizar_dual <- function(lambda, j){
    ind <- lambda < lambda[j]   
    lambda[ind] <- lambda[ind] + 1
    lambda[j] <- 1
    lambda
}

simular_proceso<- function(inicial, eleccion){
    
    Ns <- sapply(1:5, function(i) sum(eleccion==i))
    Y <- sum(1:5*Ns)

    estados <- matrix(-1, nrow=101, ncol=5)
    estados[1,] <- sapply(1:5, function(i) which(inicial==i))
        
    L <- rep(-1, 100)
    L[1] <- estados[1,eleccion[1]]
        
    for(i in 2:100){
        L[i] <- estados[i-1, eleccion[i]]
        estados[i,] <- actualizar_dual(estados[i-1,], eleccion[i])
    }
        
    list('Y'=Y, 'L'=sum(L))
        
}

simular_proceso_n <- function(n_sims){
    d_inic <- distribuciones_iniciales(n_sims)
    elecciones <- rerun(n_sims, sample(1:5, 100, prob=1:5/15, replace=TRUE))
    map2(d_inic, elecciones, simular_proceso) %>%
        map(as_data_frame) %>%
        bind_rows()
}
                          
runs <- simular_proceso_n(500)

a_gorro <- cov(runs$L, runs$Y)/var(runs$Y)

runs <- runs %>%
  mutate(Lc = L - a_gorro*(Y-1100/3))
runs %>%
  summarise(l_bar = mean(L), 
            sigma_l_hat = sd(L)) %>%
  print()
runs %>%
  summarise(lc_bar = mean(Lc), 
            sigma_lc_hat = sd(Lc)) %>%
  print()

```


## Pregunta 3

Sean X y Y dos independientes exponenciales con medias 1 y 2 respectivamente y supongan que queremos estimar $P(X+Y>4)$.¿Cómo utilizarían condicionamiento para reducir la varianza del estimador? Digan si considerarían condicionar en X o en Y y porqué.

Comencemos por realizar la simulación cruda de monte carlo:
```{r}
valor_exacto <- 2/(exp(1)^2)

x = rexp(10000,1)
y = rexp(10000,1/2)

indicadora = ifelse(x+y>4,1,0)
estimador_1 <- mean(indicadora)
estimador_1
```

Notamos que $P(X+Y>4 | X=x) = P(Y > 4-X | X=x) = 1 - P(Y < 4-X | X=x) = exp^{\frac{x}{2} -2}$.

Asimismo, $E[P(X+Y>4 | X=x)] = P(X+Y>4)$.

Usando dichos resultados, realicemos la simulación condicionando
```{r}
estimador_2 <- mean(exp(x/2 -2)) 
estimador_2
```

Y la reducción de varianza fue de:
```{r}
100*(var(exp(x/2 -2)) - var(indicadora))/var(indicadora)
```

Notemos que realizando el condicionamiento con y la estimación es sumamente mala
```{r}
estimador_3 <- mean(exp(y - 4))
estimador_3
```

Esto se debe a que $y \sim Exp(2)$ por lo que puede tomar valores de mayor magnitud que desestabilizan la simulación.

## Pregunta 4
Suponga que interesa estimar
\[
\theta = \int_0^1e^{x^2}dx
\]

Muestre que generar un número aleatorio $u$ y usar el estimador $\frac{1}{2}e^{u^2}(1+e^{1-2u})$ es mejor que generar dos números aleatorios $v_1, v_2$ y usar $\frac{1}{2}(e^{v_1^2+v_2^2})$.

```{r}
n <- 100
u <- runif(n)
v1 <- runif(n/2)
v2 <- runif(n/2)
Weu <-
theta1 <- 1/2*exp(u^2)*(1+exp(1-2*u))
c(mean(theta1), sd(theta1))

theta2 <- 1/2*exp(v1^2+v2^2)
c(mean(theta2), sd(theta2))
```

## Pregunta 5

Explicar cómo se pueden usar variables antitéticas en la simulación de la integral: $$ \int_0^1 \int_0^1 e^{(x+y)^2} dx \ dy$$
¿Es claro en este caso que usando variables antitéticas es más eficiente que generando nuevos pares de variables aleatorias? Dar una justificación a su respuesta.

El valor de dicha integral está dado por 4.89915, comencemos por realizar una simulación con el método crudo de montecarlo

```{r}
n <- 10000
x <- runif(n)
y <- runif(n)
estimador_1 <- mean(exp((x+y)^2))
varianza_1 <- var(exp(x+y)^2)
```
Obtenemos como estimador de la integral el valor:
```{r}
estimador_1
```
Con una varianza de:
```{r}
varianza_1
```

Asimismo, podemos utilizar el método de variables antitéticas de la siguiente forma: Comenzamos por generar una muestra aleatoria para las x's obteniendo $\frac{n}{2}$ uniformes $(u_1,...u_{\frac{n}{2}})$ y completamos las $\frac{n}{2}$ restantes con $\hat{u_i} = 1-u_i \ \forall i \in \{1, ... \frac{n}{2} \}$. Procedemos análogamente para generar la muestra aleatoria de las y's y realizamos la simulación. 

El resultado es el siguiente:
```{r}
x <- runif(n/2)
x <- c(x,1-x)

y <- runif(n/2)
y <- c(y,1-y)

estimador_2 <- mean(exp((x+y)^2))
varianza_2 <- var(exp((x+y)^2))
```
Obtenemos como estimador de la integral el valor:
```{r}
estimador_2
```
Con una varianza de:
```{r}
varianza_2
```

Y calculamos el porcentaje de reducción de varianza:
```{r}
100*(varianza_2 - varianza_1)/varianza_1
```

Se redujo considerablemente la varianza de forma que el método de variables antitéticas es, en este caso, más eficiente para la estimación cruda de la integral.

## Pregunta 6

En ciertas situaciones, una variable aleatoria $X$ con media conocida se simula para obtener una aproximación a $\mathbb{P}\{X\leq a\}$ para alguna constante $a$ dada. El estimador es $I = \mathbf{1}(X\leq a)$.

a. Verifique que $X$ e $I$ están negativamente correlacionadas.

\[
\begin{aligned}
\textrm{Cov}(X,I) &= \mathbb{E}[XI]-\mathbb{E}[X]\mathbb{E}[I] \\
&=\mathbb{E}[XI|I=0]\mathbb{P}\{I=0\}+E[XI|I=1]\mathbb{P}\{I=1\} - \mathbb{E}[X]\mathbb{P}\{X\leq a\} \\
&= \mathbb{E}[X|I=1]\mathbb{P}\{I=1\} - \mathbb{E}[X]\mathbb{P}\{X\leq a\} \\
&= \mathbb{E}[X|X \leq a]\mathbb{P}\{X \leq a\} - \mathbb{E}[X]\mathbb{P}\{X\leq a\} \\
&= (\mathbb{E}[X|X \leq a]-\mathbb{E}[X])\mathbb{P}\{X \leq a\}
\end{aligned}
\]

Y basta notar que el factor de la izquierda en la última línea se reescribe como 
\[
\int_{-\infty}^ax\mathbf{1}_{\mathcal{X}}(x)dF_X -  \int_{-\infty}^\infty x\mathbf{1}_{\mathcal{X}}(x)dF_X \leq 0
\]


b.  Por el inciso anterior, un intento natural de reducir la varianza es usar $X$ como variable de control (esto es usar $Y_c = I +c(X −\mathbb{E}[X])$. En este caso, determinar el porcentaje de reducción de varianza de $Y$c sobre $I$ es es posible (usando la mejor $c$ si $X$ es $\mathcal{U}(0,1)$.

Si $X \sim \mathcal{U}(0,1)$,  entonces la constante óptima es 

\[
c^*=\frac{\textrm{Cov}(I,X)}{\textrm{Var}(X)}=6(a^3-a)
\]

Y usando $I_c = I - c^*(X-\frac{1}{2})$, $\textrm{Var}(I_c) = (1-\rho^2_{X,I})\textrm{Var}(X) = (1-\frac{6\sqrt{12}(a^3-a)}{\sqrt{a-a^2}})\frac{1}{12}$

Y para ahorrarnos la simplificación, para distintos valores de $a$, tenemos

```{r}
a <- seq(from=0, to=1, length.out = 1000)

var_ic <- (1 - (6*a*(a^2-1))/(sqrt(1/12)*sqrt(a*(1-a))))*1/12
var_i <- a*(1-a)


reduccion <- (var_ic - var_i)/var_i*100
```

## Pregunta 7

El número de reclamos en una aseguradora que se harán la próxima semana depende de un factor ambiental $U$. Si el valor de este factor es $U = u$, entonces el número de reclamos tendrá distribución Poisson con media $\frac{15}{.5 + u}$. Suponiendo que
$U \sim U (0, 1)$, sea p la probabilidad de que habrá al menos 20 reclamos la siguiente semana.

Explicar como obtener una simulación cruda de p.

Comenzamos por obtener una muestra aleatoria de tamaño n de una variable aleatoria uniforme en el intervalo $[0,1]$ y posteriormente, para cada u en dicha muestra generamos una muestra de tamaño 1 de una variable aleatoria poisson con media $\frac{15}{.5 + u}$, aceptamos si dicha cantidad es mayor a 20 y rechazamos en caso contrario, Un estimador de la probabilidad requerida es la tasa de aceptación obtenida por la simulación.
```{r}
n <- 10000
u <- runif(n)
lambda <- 15/(0.5+u)
p1 <- lambda %>% map_int(function(l) rpois(1,l))
mean(p1>20)
var(p1>20)
```
Desarrollar un estimador de simulación eficiente usando esperanza condicional junto con una variable de control.

$X | U \sim Po( \lambda )$ donde $\lambda = \frac{15}{.5+u}$ 

De forma que 

\begin{eqnarray}
P(X \geq 20 | U=u) &=& 1 - P(X < 20 | U = u) \\
&=& 1 - \sum\limits_{x=0}^{19} P(X=x|U=u)  \\
&=& 1 - \sum\limits_{x=0}^{19} \frac{\lambda^x e^{-\lambda}}{x!} \quad \text{con} \quad \lambda = \frac{15}{.5+u}
\end{eqnarray}

Usando esto y dado que $E[P(X \geq 20 | U=u)] = P(X \geq 20)$ procedemos para desarrollar el estimador.
```{r}
u <- runif(n)
lambda <- 15/(0.5+u)
poiss_d <- function(x, lambda) exp(-lambda)*lambda^x/factorial(x)
p2 <- lambda %>% map_dbl(function(l) 1-sum(poiss_d(0:19, l)))
mean(p2)
var(p2)
```

Desarrollar un estimador de simulación eficiente usando esperanza condicional y variables antitéticas.
```{r}
u <- runif(n/2)
u <- c(u, 1-u)
lambda <- 15/(0.5+u)
poiss_d <- function(x, lambda) exp(-lambda)*lambda^x/factorial(x)
p3 <- lambda %>% map_dbl(function(l) 1-sum(poiss_d(0:19, l)))
mean(p3)
var(p3)
```

Notemos que la reducción de varianza con el segundo método es de:
```{r}
100*(var(p2) - var(p1))/var(p1)
```
Y con el tercer método es de:
```{r}
100*(var(p3) - var(p1))/var(p1)
```

## Pregunta 8
Considere el grafo de la tarea con matriz de adyacencia

\[
A =
\begin{pmatrix}
0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 0 \\
\end{pmatrix}
\]

y suponga que queremos estimar la longitud esperada $l$ de la ruta más corta entre los nodos $A$ y $B$, donde los arcos son variables aleatorias $X_1, \cdots , X_5$. Entonces tenemos $l = \mathbb{E}[H(X)]$, donde
\[
H(X) = \min\{ X_1 +X_4,X_1 +X_3 +X_5,X_2 +X_3 +X_4,X_2 +X_5\}
\]
Noten que $H(X)$ es no decreciente en cada componente del vector $X$. Supongan que las longitudes son independientes y $X_i ∼ \mathcal{U}(0, a_i)$, con $\mathbf{a}' = (1, 2, 3, 1, 2)$. Escribiendo $X_i = a_iU_i$ se puede restablecer el problema como la estimación de $l = \mathbb{E}(h(\mathbf{U}))$ con $h(\mathbf{U}) = H(a_1U_1,\cdots , a_5U_5)$.

a. Obtenga un estimador crudo de Monte Carlo
```{r}
a <- c(1, 2, 3, 1, 2)
X <- a %>%
  map(function(ai) runif(10000, 0, ai)) 
names(X) <- paste('X', 1:5, sep='')
X <- as_data_frame(X)

X %>%
  transmute(l = pmin(X1+X4, X1+X3+X5, X2+X3+X4, X2+X5)) %>%
  summarise(mean=mean(l), standard_deviation=sd(l))

```

b. Obtenga un estimador usando variables antitéticas
```{r}
a <- c(1, 2, 3, 1, 2)
X <- a %>%
  map(function(ai) runif(5000, 0, ai))

X <- map2(X, a, function(x,ai) c(x, ai-x))
names(X) <- paste('X', 1:5, sep='')
X <- as_data_frame(X)

X %>%
  transmute(l = pmin(X1+X4, X1+X3+X5, X2+X3+X4, X2+X5)) %>%
  summarise(mean=mean(l), standard_deviation=sd(l))
```

Obtener un estimador por variables de control
```{r}


```

## Pregunta 9

Sea S la suma de los resultados de lanzar 100 veces un dado honesto. Usen la de- sigualdad de Chebyshev para acotar P (S ≥ 380).

La desigualdad de Chebyshev dice que, dada $X$ una variable aleatoria con esperanza finita y varianza positiva, para todo número real $k>0$ tenemos: 

$$\Pr(|X-\mu |\geq k\sigma )\leq {\frac {1}{k^{2}}}.$$

A partir de dicha expresión se obtiene la siguiente desigualdad, conocida como "one-sided Chebyshev":

$$\Pr(X \geq \mu + \alpha ) \leq {\frac {\sigma^2}{\sigma^2 + \alpha^2}}.$$

Comencemos por obtener la media de S, donde S es la suma de los resultados de lanzar 100 veces un dado honesto.

$E[S] = \sum\limits_{i=1}^{100} E[X_i]$ donde $X_i$ es la variable aleatoria discreta que representa el lanzamiento de un dado honesto.

Sabemos que $E[X_i] = 3.5$ de forma que $E[S] = \sum\limits_{i=1}^{100} 3.5 = 350$

Por otro lado, $Var(S) = \sum\limits_{i=1}^{100} Var(X_i)$ dado que las $X_i$ son independientes.

Sabemos que $Var(X_i) = \frac{35}{12}$ de forma que $Var(S) = \sum\limits_{i=1}^{100} \frac{35}{12} = \frac{3500}{12}$

Utilizando estos resultados obtenemos la cota:
```{r}
mu = 350 
alpha = 30
sigma.squared = 100*(35/12)

(sigma.squared) / (sigma.squared + alpha^2)
```

Finalmente, realizamos una simulación para obtener un valor aproximado de dicha probabilidad:
```{r}
n <- 10000
sums = rep(0,n)
for (i in 1:n){
  sums[i] <- sum(sample(1:6,100, replace=T))
}
indicadora <- ifelse(sums > 380,1,0)
mean(indicadora)
```
El valor obtenido cumple, por supuesto, con la cota obenida por la desigualdad de Chebyshev pero nos permite observar que ésta última no es una cota estricta.


## Pregunta 10
Estime $\int_{-\infty}^\infty \log(x^2)e^{-x^2}dx$ por Monte Carlo crudo. Después hágalo con dos técnicas de reducción de varianza y calcule la reducción.

Notemos primero que
\[
\int_{-\infty}^\infty \log(x^2)e^{-x^2}dx = \frac{1}{\sqrt{2}}\int_{-\infty}^\infty \log\left(\frac{y^2}{2}\right)e^{-\frac{y^2}{2}}dx = \sqrt{\pi}\int_{-\infty}^\infty \log\left(\frac{y^2}{2}\right)dF
\]

donde $F \sim \mathcal{N}(0,1)$. 

```{r}
n <- 10000
u <- rnorm(n)
h <- function(x) log(x^2/2)
teta_gorro <- sqrt(pi)*mean(h(u))
teta_gorro
1/n^2*sum(h(u)-teta_gorro)^2
```

Para usar variables antitéticas recordemos que si $X\sim\mathcal{N}(0,1)$, entonces también $-X \sim \mathcal{N}(0,1)$.
```{r}
u <- rnorm(n/2)
u <- c(u, -u)
h <- function(x) log(x^2/2)
teta_gorro <- sqrt(pi)*mean(h(u))
teta_gorro
1/n^2*sum(h(u)-teta_gorro)^2
```

Y usando condicionamiento, si $Y = X^2$, entonces $\mathbb{E}[\log(X^2/2)] = \mathbb{E}[\mathbb{E}[\log(X^2/2)|Y=y]] = \mathbb{E}[\log(Y/2)]$
```{r}
y <- rchisq(n,1)
g <- function(x) log(x/2)
teta_gorro <- sqrt(pi)*mean(g(y))
teta_gorro
1/n^2*sum(g(y)-teta_gorro)^2
```